# ai_flipchart_streamlit_whisper_api.py
"""
Streamlit webâ€‘app: mikrofon âœ OpenAI Whisper API âœ ChatGPT âœ Å¾ivÃ½ â€flipchartâ€œ

âœ” UmoÅ¾Åˆuje **rychlÃ½ test** pomocÃ­ nahranÃ©ho WAV/MP3 souboru.
âœ” Funguje s `streamlit-webrtc â‰¥â€¯0.52` i na Streamlit Communityâ€¯Cloud.

LokÃ¡lnÃ­ spuÅ¡tÄ›nÃ­
----------------
1. `pip install -r requirements.txt`
2. `.streamlit/secrets.toml`Â â†’Â `OPENAI_API_KEY = "sk-â€¦"`
3. `streamlit run ai_flipchart_streamlit_whisper_api.py`

`requirements.txt`
```
streamlit
streamlit-webrtc>=0.52
openai
soundfile
numpy
```
Na Streamlit Cloud navÃ­c soubor **`packages.txt`** s jednÃ­m Å™Ã¡dkem:
```
ffmpeg
``` 
"""

from __future__ import annotations

import asyncio
import io
import json
import threading
import wave
from typing import List

import numpy as np
import streamlit as st
from openai import OpenAI
from streamlit_webrtc import WebRtcMode, webrtc_streamer

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ CONFIG â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
OPENAI_API_KEY = st.secrets.get("OPENAI_API_KEY")
if not OPENAI_API_KEY:
    st.error("ChybÃ­ OPENAI_API_KEY â€“ pÅ™idejte jej do Secrets / env vars")
    st.stop()

client = OpenAI(api_key=OPENAI_API_KEY)
AUDIO_BATCH_SECONDS = 160   # 2Â½â€“3Â min blok Å¾ivÃ©ho zvuku

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ UI â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
st.set_page_config(page_title="AI Flipchart", layout="wide")
st.title("ğŸ“‹ AI Flipchart â€“ FBW Summit 2025")

uploaded = st.file_uploader("â–¶ï¸ Nahrajte WAV/MP3 k otestovÃ¡nÃ­ (max pÃ¡r minut)",
                            type=["wav", "mp3", "m4a"], accept_multiple_files=False)

placeholder = st.empty()
if "flip_points" not in st.session_state:
    st.session_state.flip_points: List[str] = []
if "transcript_buffer" not in st.session_state:
    st.session_state.transcript_buffer = ""
if "audio_buffer" not in st.session_state:
    st.session_state.audio_buffer: list[bytes] = []

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ HELPERS â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

def pcm_frames_to_wav(frames: list[bytes], sample_rate: int = 48000) -> bytes:
    """Raw PCM int16 âœ WAV (inâ€‘memory)."""
    if not frames:
        return b""
    pcm = np.frombuffer(b"".join(frames), dtype=np.int16)
    with io.BytesIO() as buf:
        with wave.open(buf, "wb") as wf:
            wf.setnchannels(1)
            wf.setsampwidth(2)
            wf.setframerate(sample_rate)
            wf.writeframes(pcm.tobytes())
        buf.seek(0)
        return buf.read()


def summarise_new_points(text: str, existing: list[str]) -> list[str]:
    """LLM vrÃ¡tÃ­ novÃ© bulletâ€‘points (JSON pole)."""
    sys = (
        "Jsi moderÃ¡tor ÄeskÃ©ho workshopu. Z textu vyber NOVÃ‰ klÃ­ÄovÃ© myÅ¡lenky, "
        "kaÅ¾dou max 12 slov. VraÅ¥ JSON pole. Body, kterÃ© uÅ¾ jsou na flipchartu, ignoruj."
    )
    msgs = [
        {"role": "system", "content": sys},
        {"role": "user", "content": text},
        {"role": "assistant", "content": json.dumps(existing, ensure_ascii=False)},
    ]
    raw = client.chat.completions.create(
        model="gpt-3.5-turbo-1106",
        temperature=0.2,
        messages=msgs,
    ).choices[0].message.content
    try:
        pts = json.loads(raw)
        if not isinstance(pts, list):
            raise ValueError
        return [p.strip() for p in pts if p.strip()]
    except Exception:
        return [ln.lstrip("-â€¢ ").strip() for ln in raw.splitlines() if ln.strip()]

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ TEST REÅ½IM (upload) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
if uploaded is not None:
    st.info("â³ ZpracovÃ¡vÃ¡m nahranÃ½ souborâ€¦")
    transcription = client.audio.transcriptions.create(
        model="whisper-1",
        file=uploaded,
        language="cs",
    ).text

    st.session_state.flip_points = summarise_new_points(transcription, [])

    with placeholder.container():
        st.subheader("VÃ½sledek testu ğŸ“")
        for p in st.session_state.flip_points:
            st.markdown(f"- {p}")
    st.stop()

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ Å½IVÃ MIKROFON â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
webrtc_ctx = webrtc_streamer(
    key="workshop-audio",
    mode=WebRtcMode.SENDRECV,
    rtc_configuration={"iceServers": [{"urls": ["stun:stun.l.google.com:19302"]}]},
    media_stream_constraints={"audio": True, "video": False},
)

async def pipeline_runner():
    SAMPLE_RATE = 48000
    bytes_per_sec = SAMPLE_RATE * 2
    target_bytes = AUDIO_BATCH_SECONDS * bytes_per_sec

    while True:
        if not webrtc_ctx.audio_receiver:
            await asyncio.sleep(0.1)
            continue

        frames = await webrtc_ctx.audio_receiver.get_frames(timeout=1)
        st.session_state.audio_buffer.extend(fr.to_ndarray().tobytes() for fr in frames)

        if sum(len(b) for b in st.session_state.audio_buffer) < target_bytes:
            await asyncio.sleep(0.05)
            continue

        wav_bytes = pcm_frames_to_wav(st.session_state.audio_buffer)
        st.session_state.audio_buffer.clear()

        transcription = client.audio.transcriptions.create(
            model="whisper-1",
            file=io.BytesIO(wav_bytes),
            language="cs",
        ).text.strip()

        if transcription:
            st.session_state.transcript_buffer += " " + transcription

        if len(st.session_state.transcript_buffer.split()) >= 325:
            new_pts = summarise_new_points(
                st.session_state.transcript_buffer,
                st.session_state.flip_points,
            )
            st.session_state.flip_points.extend(
                [p for p in new_pts if p not in st.session_state.flip_points]
            )
            st.session_state.transcript_buffer = ""

            with placeholder.container():
                st.subheader("AktuÃ¡lnÃ­ flipchart ğŸ“")
                for p in st.session_state.flip_points:
                    st.markdown(f"- {p}")

        await asyncio.sleep(0.05)

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ start background loop â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
if "runner_created" not in st.session_state and uploaded is None:
    def _start_loop() -> None:
        loop = asyncio.new_event_loop()
        asyncio.set_event_loop(loop)
        loop.run_until_complete(pipeline_runner())

    threading.Thread(target=_start_loop, daemon=True).start()
    st.session_state.runner_created = True

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ Sidebar debug â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
st.sidebar.header("â„¹ï¸ Stav aplikace")
st.sidebar.write("Body na flipchartu:", len(st.session_state.flip_points))
st.sidebar.write("Slov v bufferu:", len(st.session_state.transcript_buffer.split()))
st.sidebar.caption("Aplikace bÄ›Å¾Ã­, dokud je karta otevÅ™enÃ¡. Pro rychlÃ½ test nahrajte audio soubor.")
