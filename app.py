# ai_flipchart_streamlit_whisper_api.py
"""
Streamlit web-app: mikrofon â†’ OpenAI Whisper â†’ ChatGPT â†’ Å¾ivÃ½ Flipchart
---------------------------------------------------------------------------

Novinky
=======
1. AnimovanÃ© vplynutÃ­ (fade-in) bodÅ¯ â€“ kaÅ¾dÃ¡ novÃ¡ odrÃ¡Å¾ka se objevÃ­ s jemnÃ½m prÅ¯letem.
2. ZÃ¡loÅ¾ky (tabs) â€“ dvÄ› karty: ğŸ›  OvlÃ¡dÃ¡nÃ­ a ğŸ“ Flipchart.
3. DiagnostickÃ½ panel ve sidebaru â€“ aktuÃ¡lnÃ­ fÃ¡ze zpracovÃ¡nÃ­.
"""

from __future__ import annotations

import asyncio
import io
import json
import threading
import wave
from typing import List

import numpy as np
import streamlit as st
from openai import OpenAI
from streamlit_webrtc import WebRtcMode, webrtc_streamer
# â€” pÅ™ipojenÃ­ kontextu, aby ve vlÃ¡knu nevznikaly varovÃ¡nÃ­ â€missing ScriptRunContextâ€œ
from streamlit.runtime.scriptrunner import add_script_run_ctx

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ CONFIG â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
OPENAI_API_KEY = st.secrets.get("OPENAI_API_KEY")
if not OPENAI_API_KEY:
    st.error("ChybÃ­ OPENAI_API_KEY â€“ pÅ™idejte jej do Secrets / env vars")
    st.stop()

client = OpenAI(api_key=OPENAI_API_KEY)
AUDIO_BATCH_SECONDS = 160

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ SESSION STATE INIT â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
if "flip_points" not in st.session_state:
    st.session_state.flip_points: List[str] = []
if "transcript_buffer" not in st.session_state:
    st.session_state.transcript_buffer = ""
if "audio_buffer" not in st.session_state:
    st.session_state.audio_buffer: list[bytes] = []
if "status" not in st.session_state:
    st.session_state.status = "ğŸŸ¡ ÄŒekÃ¡m na mikrofonâ€¦"
if "upload_processed" not in st.session_state:
    st.session_state.upload_processed = False

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ CSS (fade-in + fullscreen) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
STYLES = """
<style>
ul.flipchart {list-style-type:none; padding-left:0;}
ul.flipchart li {opacity:0; transform:translateY(8px); animation:fadeIn 0.45s forwards;}
@keyframes fadeIn {to {opacity:1; transform:translateY(0);}}
.fullscreen header, .fullscreen #MainMenu, .fullscreen footer {visibility:hidden;}
.fullscreen .block-container {padding-top:0.5rem;}
</style>
"""

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ HELPERS â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

def set_status(s: str) -> None:
    """ZapÃ­Å¡e stav jen tehdy, pokud se zmÄ›nil â€“ Å¡etÅ™Ã­ log."""
    if st.session_state.status != s:
        st.session_state.status = s


def render_flipchart() -> None:
    """VykreslÃ­ flipchart s animacÃ­."""
    st.markdown(STYLES, unsafe_allow_html=True)
    points = st.session_state.flip_points
    if not points:
        st.info("ÄŒekÃ¡m na prvnÃ­ shrnutÃ­â€¦")
        return
    bullets = "<ul class='flipchart'>" + "".join(
        [f"<li style='animation-delay:{i*0.1}s'>{p}</li>" for i, p in enumerate(points)]
    ) + "</ul>"
    st.markdown(bullets, unsafe_allow_html=True)


def pcm_frames_to_wav(frames: list[bytes], sample_rate: int = 48000) -> bytes:
    """SlouÄÃ­ PCM rÃ¡mce do WAV souboru."""
    if not frames:
        return b""
    pcm = np.frombuffer(b"".join(frames), dtype=np.int16)
    with io.BytesIO() as buf:
        with wave.open(buf, "wb") as wf:
            wf.setnchannels(1)
            wf.setsampwidth(2)
            wf.setframerate(sample_rate)
            wf.writeframes(pcm.tobytes())
        buf.seek(0)
        return buf.read()


PROMPT = """
Jsi zkuÅ¡enÃ½ moderÃ¡tor workshopu FWB Summit 2025. CholnÃ© setkÃ¡nÃ­ podnikatelskÃ½ch rodin,
expertÅ¯, akademikÅ¯ a politikÅ¯, kteÅ™Ã­ sdÃ­lÃ­ zkuÅ¡enosti a formujÃ­ budoucnost rodinnÃ©ho
podnikÃ¡nÃ­.

TvÃ½m Ãºkolem je shrnovat projev do klÃ­ÄovÃ½ch myÅ¡lenek v tomto formÃ¡tu:

"NADPIS MYÅ LENKY
- detail 1
- detail 2
- detail 3"
[â€¦]

Z textu vyber **novÃ©** klÃ­ÄovÃ© myÅ¡lenky a vraÅ¥ je jako JSON pole. Body, kterÃ© uÅ¾ jsou
na flipchartu, ignoruj.
"""

def summarise_new_points(text: str, existing: list[str]) -> list[str]:
    """ZavolÃ¡ ChatGPT a vrÃ¡tÃ­ novÃ© body, kterÃ© zatÃ­m nejsou na flipchartu."""
    msgs = [
        {"role": "system", "content": PROMPT},
        {"role": "user", "content": text},
        {"role": "assistant", "content": json.dumps(existing, ensure_ascii=False)},
    ]
    raw = client.chat.completions.create(
        model="gpt-3.5-turbo-1106", temperature=0.2, messages=msgs
    ).choices[0].message.content
    try:
        pts = json.loads(raw)
        if not isinstance(pts, list):
            raise ValueError
        return [p.strip() for p in pts if p.strip()]
    except Exception:
        # fallback â€“ rozparsuj prostÃ½ text po Å™Ã¡dcÃ­ch
        return [ln.lstrip("-â€¢ ").strip() for ln in raw.splitlines() if ln.strip()]

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ STREAMLIT LAYOUT (tabs) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
st.set_page_config(layout="wide", page_title="AI Flipchart")

tabs = st.tabs(["ğŸ›  OvlÃ¡dÃ¡nÃ­", "ğŸ“ Flipchart"])

# ========== Tab 1: OVLÃDÃNÃ ==========
with tabs[0]:
    st.header("NastavenÃ­ a vstup zvuku")

    uploaded = st.file_uploader(
        "â–¶ï¸ Nahrajte WAV/MP3 k otestovÃ¡nÃ­ (max pÃ¡r minut)",
        type=["wav", "mp3", "m4a"],
        accept_multiple_files=False,
    )

    # â€” jednorÃ¡zovÃ© zpracovÃ¡nÃ­ uploadu, bez st.stop()
    if uploaded is not None:
        if not st.session_state.upload_processed:
            st.info("â³ ZpracovÃ¡vÃ¡m nahranÃ½ souborâ€¦")
            transcription = client.audio.transcriptions.create(
                model="whisper-1", file=uploaded, language="cs"
            ).text
            new_pts = summarise_new_points(transcription, st.session_state.flip_points)
            st.session_state.flip_points.extend(new_pts)
            st.session_state.upload_processed = True
            st.success("âœ… Soubor zpracovÃ¡n â€“ pÅ™epnÄ›te na zÃ¡loÅ¾ku Flipchart")
        else:
            st.success("âœ… Soubor uÅ¾ byl zpracovÃ¡n â€“ pÅ™epnÄ›te na zÃ¡loÅ¾ku Flipchart")

    # ---- Live microphone
    st.subheader("ğŸ¤ Å½ivÃ½ mikrofon")
    webrtc_ctx = webrtc_streamer(
        key="workshop-audio",
        mode=WebRtcMode.SENDRECV,
        rtc_configuration={"iceServers": [{"urls": ["stun:stun.l.google.com:19302"]}]},
        media_stream_constraints={"audio": True, "video": False},
    )

    async def pipeline_runner():
        SAMPLE_RATE = 48000
        bytes_per_sec = SAMPLE_RATE * 2
        target_bytes = AUDIO_BATCH_SECONDS * bytes_per_sec
        while True:
            if not webrtc_ctx.audio_receiver:
                set_status("ğŸŸ¡ ÄŒekÃ¡m na mikrofonâ€¦")
                await asyncio.sleep(0.1)
                continue

            set_status("ğŸ”´ ZachytÃ¡vÃ¡m audioâ€¦")
            frames = await webrtc_ctx.audio_receiver.get_frames(timeout=1)
            st.session_state.audio_buffer.extend(fr.to_ndarray().tobytes() for fr in frames)

            if sum(len(b) for b in st.session_state.audio_buffer) < target_bytes:
                await asyncio.sleep(0.05)
                continue

            set_status("ğŸŸ£ OdesÃ­lÃ¡m do Whisperâ€¦")
            wav_bytes = pcm_frames_to_wav(st.session_state.audio_buffer)
            st.session_state.audio_buffer.clear()

            transcription = client.audio.transcriptions.create(
                model="whisper-1", file=io.BytesIO(wav_bytes), language="cs"
            ).text
            st.session_state.transcript_buffer += " " + transcription

            if len(st.session_state.transcript_buffer.split()) >= 325:
                set_status("ğŸ§  Generuji shrnutÃ­â€¦")
                new_pts = summarise_new_points(
                    st.session_state.transcript_buffer,
                    st.session_state.flip_points,
                )
                st.session_state.flip_points.extend(
                    [p for p in new_pts if p not in st.session_state.flip_points]
                )
                st.session_state.transcript_buffer = ""

            set_status("ğŸŸ¢ ÄŒekÃ¡m na dalÅ¡Ã­ dÃ¡vkuâ€¦")
            await asyncio.sleep(0.05)

    # â€” spuÅ¡tÄ›nÃ­ vlÃ¡kna s kontextem Streamlitu
    if "runner_created" not in st.session_state:
        t = threading.Thread(
            target=lambda: asyncio.run(pipeline_runner()),
            daemon=True,
            name="audio-runner",
        )
        add_script_run_ctx(t)   # dÅ¯leÅ¾itÃ© kvÅ¯li ScriptRunContext
        t.start()
        st.session_state.runner_created = True

    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ SIDEBAR â€“ diagnostika â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    st.sidebar.header("â„¹ï¸ Stav aplikace")
    st.sidebar.write("Body na flipchartu:", len(st.session_state.flip_points))
    st.sidebar.write("Slov v bufferu:", len(st.session_state.transcript_buffer.split()))
    st.sidebar.subheader("ğŸ§­ Stav zpracovÃ¡nÃ­")
    st.sidebar.write(st.session_state.get("status", "â” NeznÃ¡mÃ½ stav"))

# ========== Tab 2: FLIPCHART ==========
with tabs[1]:
    st.components.v1.html(
        "<script>document.body.classList.add('fullscreen');</script>", height=0
    )
    render_flipchart()
