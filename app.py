# audio_upload_whisper_segmenter_live.py
"""
Streamlit app ‚Äì¬†**revamp podle whitphx demo**
==========================================
1. **Upload** ‚Äì¬†statick√© nahr√°n√≠ ‚Üí Whisper ‚Üí Make ‚Üí bullet‚Äëpoints.
2. **Live (WebRTC)** ‚Äì mic / virtu√°ln√≠ kabel zachycen p≈ô√≠mo v¬†UI vl√°knu, _bez_ autorefreshu ƒçi rerun≈Ø:
   * `webrtc_streamer` (SENDONLY)
   * v¬†samostatn√©m vl√°knƒõ ƒçteme `audio_receiver.get_frames()`
   * ka≈æd√Ωch `SEGMENT_SEC` => WAV¬†‚Üí¬†Whisper¬†‚Üí¬†Make ‚Üí p≈ôid√° se do session_state ‚Üí UI se aktualizuje p≈ôes `st.session_state.changed` + jemn√Ω `st.rerun()` **jen** po¬†p≈ô√≠chodu nov√Ωch dat¬†‚Üí ≈æ√°dn√© blik√°n√≠.
   * Realtime RMS bar.
"""
from __future__ import annotations

import asyncio, io, time, threading, queue, wave, logging
from typing import List

import av, numpy as np, streamlit as st, openai, requests
from streamlit_webrtc import webrtc_streamer, WebRtcMode, RTCConfiguration

# ----------------------------------------------------------------------------
# CONFIG ---------------------------------------------------------------------
# ----------------------------------------------------------------------------
SEGMENT_SEC       = 60
WHISPER_MODEL     = "whisper-1"
MAKE_WEBHOOK_URL  = st.secrets.get("MAKE_WEBHOOK_URL", "")
OPENAI_API_KEY    = st.secrets.get("OPENAI_API_KEY", "")

client = openai.OpenAI(api_key=OPENAI_API_KEY)

logging.basicConfig(level=logging.INFO)

# ----------------------------------------------------------------------------
# Helper functions ------------------------------------------------------------
# ----------------------------------------------------------------------------

def pcm_to_wav(pcm: bytes, sr: int) -> bytes:
    buf = io.BytesIO()
    with wave.open(buf, "wb") as w:
        w.setnchannels(1)
        w.setsampwidth(2)
        w.setframerate(sr)
        w.writeframes(pcm)
    return buf.getvalue()


def whisper_transcribe(wav_bytes: bytes) -> str:
    r = client.audio.transcriptions.create(
        model=WHISPER_MODEL,
        file=("live.wav", io.BytesIO(wav_bytes), "audio/wav"),  # type: ignore[arg-type]
    )
    return r.text  # type: ignore[attr-defined]


def post_to_make(transcript: str) -> List[str]:
    if not MAKE_WEBHOOK_URL:
        return ["‚ö†Ô∏è MAKE_WEBHOOK_URL nen√≠ v Secrets"]
    resp = requests.post(MAKE_WEBHOOK_URL, json={"transcript": transcript}, timeout=30)
    resp.raise_for_status()
    return resp.json().get("bullets", [])

# ----------------------------------------------------------------------------
# Background worker -----------------------------------------------------------
# ----------------------------------------------------------------------------

def start_worker(ctx, placeholders):
    """Spust√≠ thread, kter√Ω tah√° AudioFrame‚Äëy a¬†p≈ôen√°≈°√≠ v√Ωsledky do session_state."""

    if "worker_running" in st.session_state:  # u≈æ bƒõ≈æ√≠
        return

    def _worker():
        audio_buf = bytearray()
        last      = time.time()
        sr        = 48000
        while ctx.state.playing:
            try:
                frames = ctx.audio_receiver.get_frames(timeout=1)
            except queue.Empty:
                continue

            for frame in frames:
                if not isinstance(frame, av.AudioFrame):
                    continue
                pcm16 = frame.to_ndarray().tobytes()
                audio_buf.extend(pcm16)
                sr = frame.sample_rate or sr

                # RMS ‚Üí level bar
                rms = float(np.sqrt(np.mean(np.square(np.frombuffer(pcm16, np.int16)))) / 32768)
                st.session_state["_audio_rms"] = rms

            # ka≈æd√Ωch SEGMENT_SEC ‚Üí Whisper ‚Üí Make
            if time.time() - last >= SEGMENT_SEC and audio_buf:
                wav_bytes = pcm_to_wav(bytes(audio_buf), sr)
                audio_buf.clear(); last = time.time()
                try:
                    txt     = whisper_transcribe(wav_bytes)
                    bullets = post_to_make(txt)
                except Exception as e:  # network / API error
                    bullets = [f"‚ùå {e}"]
                st.session_state.setdefault("bullets", []).extend(bullets)
                st.experimental_rerun()  # jedin√Ω rerun ‚Äì jen kdy≈æ m√°me nov√© bullet‚Äëpoints

        st.session_state.pop("worker_running", None)

    threading.Thread(target=_worker, daemon=True).start()
    st.session_state["worker_running"] = True

# ----------------------------------------------------------------------------
# UI --------------------------------------------------------------------------
# ----------------------------------------------------------------------------

def main():
    st.set_page_config("AI Moder√°tor", "üìù", layout="centered")
    st.title("üìù AI Moder√°tor ‚Äì audio ‚ûú bullet‚Äëpoints")

    st.sidebar.header("Nastaven√≠")
    mode = st.sidebar.radio("Re≈æim", ["Upload", "Live"], index=1)
    seg  = st.sidebar.slider("Interval segmentu (s)", 15, 180, SEGMENT_SEC, 5)
    globals()["SEGMENT_SEC"] = seg

    if mode == "Upload":
        upload_ui()
    else:
        live_ui()

# --------------------------------------------

def upload_ui():
    f = st.file_uploader("Nahraj audio", type=["wav", "mp3", "m4a"])
    if f and st.button("Zpracovat"):
        with st.spinner("Whisper ‚Üí Make‚Ä¶"):
            txt = whisper_transcribe(f.read())
            bullets = post_to_make(txt)
        st.subheader("Bullet‚Äëpoints")
        st.markdown("\n".join(f"‚Ä¢ {b}" for b in bullets))

# --------------------------------------------

def live_ui():
    st.markdown("Klikni **Allow** pro mikrofon / virtu√°ln√≠ kabel.")

    rtc_cfg: RTCConfiguration = {
        "iceServers": [
            {"urls": ["stun:stun.l.google.com:19302"]},
            {"urls": "turn:global.relay.metered.ca:80", "username": "global", "credential": "global"},
        ]
    }

    ctx = webrtc_streamer(
        key="live",
        mode=WebRtcMode.SENDONLY,
        media_stream_constraints={"audio": True, "video": False},
        rtc_configuration=rtc_cfg,
        async_processing=True,
    )

    level_ph = st.empty()
    bullet_ph = st.container()

    # start background worker once the stream is up
    if ctx.state.playing and ctx.audio_receiver:
        start_worker(ctx, (level_ph, bullet_ph))

    # render level bar (updates in place ‚Äì no rerun needed)
    rms = st.session_state.get("_audio_rms", 0.0)
    bar_len = int(rms * 20)
    bar = "üü©" * bar_len + "‚ñ´Ô∏è" * (20 - bar_len)
    level_ph.markdown(f"**√örove≈à audia:** {bar}")

    # render bullets (after possible rerun)
    bullets: List[str] = st.session_state.get("bullets", [])
    bullet_ph.subheader("≈Ωiv√© bullet‚Äëpoints")
    if bullets:
        bullet_ph.markdown("\n".join(f"‚Ä¢ {b}" for b in bullets))
    else:
        bullet_ph.info("ƒåek√°m na prvn√≠ segment‚Ä¶")

# ----------------------------------------------------------------------------
if __name__ == "__main__":
    main()
